\documentclass{article}
\usepackage{mytitlesec}

\usepackage{mytitlesec}
\titlespacing\section{0pt}{0pt plus 0pt minus 0pt}{0pt plus 0pt minus 0pt}
\titlespacing\subsection{0pt}{0pt plus 0pt minus 0pt}{0pt plus 0pt minus 0pt}
\titlespacing\subsubsection{0pt}{0pt plus 0pt minus 0pt}{0pt plus 0pt minus 0pt}
% \setlength{\abovedisplayskip}{-8pt}
% \setlength{\belowdisplayskip}{-8pt}
% \setlength{\abovedisplayshortskip}{-20pt}
% \setlength{\belowdisplayshortskip}{-20pt}

\usepackage{newtxtext,newtxmath}
% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_data_2023

% ready for submission
%\usepackage{neurips_data_2024}

% to compile a preprint version, add the [preprint] option, e.g.:
%     \usepackage[preprint]{neurips_data_2023}
% This will indicate that the work is currently under review.

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_data_2024}

% to avoid loading the natbib package, add option nonatbib:
% usepackage[nonatbib]{neurips_data_2023}

% Submissions to the datasets and benchmarks are typically non anonymous,
% but anonymous submissions are allowed. If you feel that you must submit 
% anonymously, you can compile an anonymous version by adding the [anonymous] 
% option, e.g.:
%     \usepackage[anonymous]{neurips_data_2023}
% This will hide all author names.
\newcommand{\jxs}[1]{{\color{red} JXS: #1}}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[breaklinks=true,bookmarks=true,colorlinks,pagebackref=true]{hyperref}     % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{bbding}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving}

\author{%
  Xiaosong Jia* \And Zhenjie Yang*  \And Qifeng Li* \And Zhiyuan Zhang*   \And Junchi Yan$^\dagger$ \\
  \normalsize{$^*$ Equal contributions.  \quad $^\dagger$ Correspondence author} \\ \\
  Dept. of CSE \& School of AI \& MoE Key Lab of AI, Shanghai Jiao Tong University\\ \\
    \normalsize{
    \url{https://thinklab-sjtu.github.io/Bench2Drive/}
    }
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
   In an era marked by the rapid scaling of foundation models, autonomous driving technologies are approaching a transformative threshold where end-to-end autonomous driving (E2E-AD) emerges due to its potential of scaling up in the data-driven manner. However, existing E2E-AD methods are mostly evaluated under the open-loop log-replay manner with L2 errors and collision rate as metrics (e.g., in nuScenes), which could not fully reflect the driving performance of algorithms as recently acknowledged in the community. For those E2E-AD methods evaluated under the closed-loop protocol, they are tested in fixed routes (e.g., Town05Long and Longest6 in CARLA) with the driving score as metrics, which is known for high variance due to the unsmoothed metric function and large randomness in the long route. Besides, these methods usually collect their own data for training, which makes algorithm-level fair comparison infeasible. 

   To fulfill the paramount need of comprehensive, realistic, and fair testing environments for Full Self-Driving (FSD), we present \textbf{Bench2Drive}, the first benchmark for evaluating E2E-AD systems' multiple abilities in a closed-loop manner. Bench2Drive's official training data consists of 2 million fully annotated frames, collected from 13638  short clips uniformly distributed under 44 interactive scenarios (cut-in, overtaking, detour, etc), 23 weathers (sunny, foggy, rainy, etc), and 12 towns (urban, village, university, etc) in CARLA v2. Its evaluation protocol requires E2E-AD models to pass 44 interactive scenarios under different locations and weathers which sums up to 220 routes and thus provides a comprehensive and disentangled assessment about their driving capability under different situations. We implement state-of-the-art E2E-AD models and evaluate them in Bench2Drive, providing insights regarding current status and future directions.
\end{abstract}


\section{Introduction}
\begin{figure}
    \centering
    \includegraphics[width=14.0cm]{fig/overview.jpg}
    \caption{\textbf{Overview of Bench2Drive}.}
    \label{fig:overview}\vspace{-4mm}
\end{figure}

\renewcommand{\thefootnote}{} % Temporarily remove numbering for this footnote
\footnotetext{This work was in part supported by by NSFC (92370201, 62222607) and Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0102.} % Footnote text
\renewcommand{\thefootnote}{\arabic{footnote}}
In recent years, the field of autonomous driving has witnessed tremendous growth, fueled by the rapid advancement and scaling of foundation models~\cite{bommasani2021opportunities,Achiam2023GPT4TR,Yang2023LLM4DriveAS}. These developments have ushered in a new era of end-to-end autonomous driving (E2E-AD) systems~\cite{hu2023planning,wu2022trajectory,Chitta2023PAMI,xu2020siamfc++,xu2023drl}, which promise a scalable, data-driven approach to vehicle automation, opposed to traditional module-based perception~\cite{li2022bevsurvey,li2022bevformer,liu2022bevfusion,huang2023iddr,zhu2024flatfusiondelvingdetailssparse}, prediction~\cite{jia2022multi,jia2023towards,jia2023hdgt,jia2024amp}, planning~\cite{ide-net,Dauner2023CORL,niu2024lightzero} pipeline. Such systems are designed to be capable of learning from vast amounts of data, potentially transforming the landscape of vehicle intelligence.


Despite these advancements, \textbf{the evaluation methodologies for E2E-AD systems remain a critical bottleneck}. One popular way is to conduct log-replay with the recorded expert trajectories in dataset like nuScenes~\cite{nuscenes}, i.e., open-loop evaluation. These models~\cite{hu2023planning,jiang2023vad} usually predict the future locations of the ego vehicle with the raw sensor information as inputs. As for metrics,  the L2 error relative to the recorded trajectories and the ratio of collision happening are used. However, as widely discussed in the community~\cite{zhai2023ADMLP,Li2023IsES,Dauner2023CORL}, these open-loop metrics are insufficient for showcasing proficiency in planning, due to issues including distribution shift~\cite{ross2011reduction}, causal confusion~\cite{de2019causal,jia2023driveadapter}, etc. nuScenes is also problematic due to its small and imbalanced validation set (around 75\% of the frames only require continuing to drive straight)~\cite{Li2023IsES}. As a result, only encoding the ego status (location, speed, etc)~\cite{zhai2023ADMLP} could achieve similar L2 errors compared to complex methods with sensor inputs~\cite{hu2023planning}, which \textbf{prompts a call for a closed-loop evaluation benchmark for E2E-AD}.

CARLA~\cite{dosovitskiy2017carla} is one of the most widely used simulator for closed-loop E2E-AD evaluation. Within its framework, benchmarks such as Town05Long and Longest6 have been established, featuring multiple routes that require AD systems to complete safely within specific time constraints. However, these benchmarks only assess basic skills such as lane following, making turns, collision avoidance, and traffic lights obeying~\cite{chen2022learning,jia2023thinktwice}, \textbf{failing to examine AD systems' driving ability under complicated and interactive traffic}. The latest CARLA Leaderboard v2 introduces 39 challenging scenarios designed to evaluate the robustness of AD systems in more intricate situations. Nevertheless,  the official routes for evaluation, ranging from 7 to 10 kilometers and filled with scenarios, present a formidable challenge, often too difficult to complete flawlessly, as shown in Fig.~\ref{fig:route-length} (a). Consequently, with the driving score metric employing an exponential decay function, it becomes challenging to effectively compare different AD systems, as they tend to score very low. For instance, in the current Leaderboard v2\footnote{\url{https://eval.ai/web/challenges/challenge-page/2098/leaderboard/4942}},  participating methods score less than 10 points out of 100. Besides, existing methods usually collect data by themselves which makes algorithm-level fair comparison infeasible.

\begin{figure}[tb!]
    \centering
    \begin{subfigure}[t]{0.45\linewidth}
        \includegraphics[width=\linewidth]{fig/official_cover.png}
        \caption{CARLA Leaderboard V2}
      \end{subfigure}
      \begin{subfigure}[t]{0.45\linewidth}
          \includegraphics[width=\linewidth]{fig/our_cover.png}
          \caption{Bench2Drive Dataset}
      \end{subfigure}
    \caption{\textbf{Route length on Town12.} We use different colors to represent different routes. Bench2Drive's short routes provide more smoothed evaluations.\label{fig:route-length}}\vspace{-6mm}
\end{figure}

To address the aforementioned challenges in evaluating autonomous driving (AD) systems, it is essential to develop a new benchmark that fairly assesses their capabilities in a granular manner. To this end, we introduce \textbf{Bench2Drive}, a new benchmark designed to evaluate E2E-AD systems in a comprehensive, realistic, and fair closed-loop environment. Bench2Drive has \textbf{an official training dataset}  collected by state-of-the-art expert model \textit{Think2Drive}~\cite{li2024think2drive}, comprising 2 million fully annotated frames, sourced from 13638  clips. It span a diverse array of 44 interactive scenarios such as cut-ins, overtakings, and detours under different weather conditions and towns, ranging from sunny days in bustling city centers to foggy conditions in quaint villages. \textbf{The evaluation protocol includes 220 short routes}, each only around 150 meters in length and containing a single specific scenario. In this way, the assessment of individual skills is isolated and thus allows for a detailed comparison of the AD systems' proficiency across 44 distinct skill sets. Moreover, the brevity of each route mitigates the impact of the exponential decay function on the driving score, facilitating a more accurate and meaningful comparison of performance across different systems. Such a structured and focused benchmark would provide clearer insights into the strengths and weaknesses of each AD system, enabling targeted improvements and more refined technology development.


In summary, the proposed Bench2Drive benchmark features:



\begin{itemize}[leftmargin=10pt, topsep=0pt, itemsep=1pt, partopsep=1pt, parsep=1pt]
    \item \textbf{Comprehensive Scenario Coverage}: Bench2Drive is designed to test AD systems across 44 interactive scenarios, providing a thorough evaluation about capabilities under complex situations.
    \item \textbf{Granular Skill Assessment}: By structuring the evaluation across 220 short routes, each focusing on a specific driving scenario, Bench2Drive allows for detailed analysis and comparison of how different AD systems perform on individual tasks.
    \item  \textbf{Closed-Loop Evaluation Protocol}: Bench2Drive evaluates AD systems in a closed-loop manner, where the AD system's actions directly influence the environment. This setup offers an accurate assessment of an AD system's driving performance.
    
    \item \textbf{Diverse Large-Scale Official Training Data}: Bench2Drive consists of a standardized training set of 2 million fully annotated frames from 13638 clips under diverse scenarios, weathers, and towns, ensuring that all AD systems are trained under abundant yet similar conditions, which is crucial for fair algorithm-level comparisons.
\end{itemize}


\emph{These features make Bench2Drive a pioneering benchmark in the field of autonomous driving, providing an essential tool for researchers to refine and evaluate their E2E-AD systems in a realistic, comprehensive, and fair manner.} \textbf{We implement several classic baselines including TCP~\cite{wu2022trajectory}, ThinkTwice~\cite{jia2023thinktwice}, DriveAdapter~\cite{jia2023driveadapter}, UniAD~\cite{hu2023planning}, VAD~\cite{jiang2023vad}, and AD-MLP~\cite{zhai2023ADMLP} and evaluate them in the Bench2Drive}. We confirm the fact that open-loop metrics like L2 error could not reflect the actual driving performance. For the classic closed-loop metric - Drive Score, we find that it lack details and its heavy punishment encourages over-conservative driving strategies while Bench2Drive offers a comprehensive understanding about capabilities of different methods.


\section{Related Work}
\subsection{Planning Benchmarks}
Benchmarking in the field of autonomous driving has evolved from specialized datasets, such as KITTI~\cite{Geiger2012CVPR} for perception and NGSIM/highD~\cite{krajewski2018highd}, BARK~\cite{bernhard2020bark} for behavior prediction, to integrated forms like nuScenes~\cite{nuscenes}, Argoverse~\cite{Argoverse2}, and Waymo~\cite{sun2020scalability}, which facilitate the evaluation of various synergic system components. Recently, the assessment of planning capabilities for learning-based methods has become an area of  interest~\cite{gulino2024waymax,karnchanachari2024towards,NEURIPS2022_8be9c134, Bai_Zhang_Tao_Wu_Wang_Xu_2023, bai2024efficient}. In Table~\ref{tab:comparision}, we present a comparison of  planning benchmarks. nuScenes~\cite{nuscenes}, while offering open-loop metrics, has been critiqued for its inability to adequately evaluate planning proficiency due to the lack of closed-loop simulation~\cite{zhai2023ADMLP,Li2023IsES,Dauner2023CORL}. Furthermore, it suffers from an imbalanced validation set, with a significant portion (75\%) of scenarios only requiring straightforward driving, thus inadequately challenging the decision-making capabilities of AD systems in complex environments~\cite{Li2023IsES}.  nuPlan~\cite{karnchanachari2024towards} and Waymax~\cite{gulino2024waymax} offer closed-loop evaluations but are limited to bounding box level assessments, excluding sensor simulation and, consequently, are not suitable for E2E-AD methods. Longest6~\cite{Chitta2023PAMI}, a modified version of CARLA Leaderboard V1, only assesses basic skills such as lane following, making turns, collision avoidance, and traffic lights. CARLA Leaderboard V2~\cite{dosovitskiy2017carla}  lacks expert demonstration data.  As widely discussed in the community~\cite{Renz2022CORL,Jaeger2023ICCV}, the lack of an official training set makes the comparisons of different methods in the system-level instead of the algorithm-level.Bench2Drive deal with these shortcomings by offering a large-scale, annotation-rich official training dataset alongside a multi-ability evaluation set. This enables a more granular and informative assessment of an AD system's driving capabilities, overcoming the limitations of existing benchmarks that rely on average scoring across all routes as their primary performance metric.



\begin{table}[]
\caption{\textbf{Comparison with related planning benchmarks} Bench2Drive is the only benchmark to evaluate the E2E-AD methods under closed-loop with multi-ability analysis.\label{tab:comparision}}
\scalebox{0.9}{
\begin{tabular}{c|cccccccc}
\toprule
\textbf{Benchmark}    & \textbf{Sensor} & \textbf{Closed-Loop} & \textbf{E2E-Sim} & \textbf{Expert} & \textbf{Complex} & \textbf{Multi-Ability-Eval} \\ \midrule
nuScenes~\cite{nuscenes}             &  \Checkmark                  &    \XSolidBrush               &       \XSolidBrush                  & \Checkmark                          &   \XSolidBrush    & \XSolidBrush             \\
nuPlan~\cite{karnchanachari2024towards}              &  \Checkmark                  &    \Checkmark               &       \XSolidBrush                  & \Checkmark                          &   \Checkmark    & \XSolidBrush             \\
Waymax~\cite{gulino2024waymax}                &  \XSolidBrush                  &    \Checkmark               &       \XSolidBrush                  & \Checkmark                          &   \Checkmark    & \XSolidBrush             \\
Longest6~\cite{Chitta2023PAMI} &  \Checkmark                  &    \Checkmark               &       \Checkmark                  & \Checkmark                          &   \XSolidBrush    & \XSolidBrush             \\
CARLA LB V2~\cite{dosovitskiy2017carla} &  \Checkmark                  &    \Checkmark               &       \Checkmark                  & \XSolidBrush                          &   \Checkmark    & \XSolidBrush             \\ \midrule
\textbf{Bench2Drive (Ours)}    &  \Checkmark                  &    \Checkmark               &       \Checkmark                  & \Checkmark                          &   \Checkmark    & \Checkmark             \\  \bottomrule
\end{tabular}}\vspace{-4mm}
\end{table}


\subsection{End-to-End Autonomous Driving}
The concept of E2E-AD could date back to 1980s~\cite{pomerleau1988alvinn}. Recently, the arise of neural network, especially Transformer~\cite{Vaswani2017AttentionIA}, demonstrates the power of scaling laws, which rejuvenates the enthusiasm for E2E-AD~\cite{codevilla2018cil,codevilla2019exploring,toromanoff2020end,chen2020learning,zhang2021roach}. However, they are either evaluated only in the open-loop way~\cite{hu2022stp3,hu2023planning,jiang2023vad,lu2024activead} or in the relatively simple scenes like Town05Long/Longest6~\cite{chitta2021neat,Prakash2021CVPR,Renz2022CORL,wu2023PPGeo,chen2022lav,shao2022interfuser,hu2022model,zhang2022mmfn,zhang2023coaching}. Bench2Drive offers a challenging and comprehensive arena to compare  E2E-AD methods' ability.

\section{Bench2Drive}
\label{sec:method}
Bench2Drive consists of a large-scale fully annotated dataset collected in CARLA as the official training set, an evaluation toolkit for the granular driving skill assessment, and implementations of several state-of-the-art E2E-AD methods tailored for the training dataset and evaluation toolkit. All data, codes, and checkpoints are in GitHub and Huggingface under Apache License 2.0.  We give details in the following section.

\subsection{Data Collection Agent}
The data collection agent (expert) is responsible for collecting the data so that student models could learn from the data. In the real world, this is usually done by human to drive around the city, like the curation of KITTI~\cite{Geiger2012CVPR}, nuScenes~\cite{nuscenes}, Waymo~\cite{sun2020scalability}, Argoverse~\cite{Argoverse2}. However, it requires lots of human efforts. In simulation, there is a cheap substitute - teacher model. The teacher model would use information not available in the real world (termed privileged information), for example, ground-truth locations, states, and intentions of surrounding agents and ground-truth states of traffic lights, etc. As a result, people using CARLA either write rules~\cite{Jaeger2023ICCV,Beibwenger2024PdmLite} or train a RL model~\cite{zhang2021roach,li2024think2drive} to use the privileged information to drive in the simulation. 

In this work, we use the world model based reinforcement learning teacher - Think2Drive~\cite{li2024think2drive} to navigate in CARLA and collect data, since \textbf{it is the only expert model which is able to solve all 44 scenarios during the construction of Bench2Drive}. Notably, after the release of Bench2Drive, the rule-based expert PDM-Lite~\cite{Beibwenger2024PdmLite}\footnote{\url{https://github.com/autonomousvision/carla_garage/tree/leaderboard_2}} is open sourced and users could use it for customized demand.



\subsection{Expert Dataset}
Existing E2E-AD methods evaluated in the closed-loop manner~\cite{wu2022trajectory,Chitta2023PAMI,chen2022lav,shao2022interfuser} typically collect their own data using the CARLA simulator.  However, as highlighted in~\cite{Renz2022CORL,Jaeger2023ICCV}, the size and distributions of these datasets  significantly influence performance, rendering fair algorithm-level comparisons challenging.  To address this, we have constructed a large-scale expert dataset with comprehensive annotations including 3D bounding boxes, depth, and semantic segmentation, sampled at 10 Hz, to serve as the official training set. As the information from expert could be an important guidance of student models~\cite{chen2020learning,jia2023driveadapter,zhang2023coaching}, we also provide the expert model - Think2Drive's~\cite{li2024think2drive} value estimation and features.
Fig.~\ref{fig:sensor-anno} gives an overview. To facilitate the re-implementation of existing E2E-AD methods of community, we adopt a sensor configuration similar to nuScenes~\cite{nuscenes}:
\begin{itemize}[leftmargin=10pt, topsep=0pt, itemsep=1pt, partopsep=1pt, parsep=1pt]
    \item 1x \textbf{LiDAR}: 64 channels, 85-meter range, 600,000 points per second
    \item 6x \textbf{Camera}: Surround coverage, 900x1600 resolution, JPEG compression (quality-level 20)
    \item 5x \textbf{Radar}: 100-meter range, $30^{\circ}$ horizontal and vertical FoV
    \item 1x \textbf{IMU \& GNSS}: Location, yaw, speed, acceleration, and angular velocity
    \item 1x \textbf{BEV Camera}: Debugging, visualization, remote sensing
    \item \textbf{HD-Map}: Lanes, centerlines, topology, dynamic light states, trigger areas for lights and stop signs
\end{itemize}




\begin{figure}[tb!]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/sensor_annotation.pdf}
    \caption{\textbf{Sensor setting and annotations of the expert dataset.} We follow the sensor settings of nuScenes~\cite{nuscenes}. The annotations include 3D bounding boxes, depth, semantic/instance segmentation, HD-Map, and RL value estimations and features from Think2Drive~\cite{li2024think2drive} Expert.}
    \label{fig:sensor-anno}
\end{figure}


\begin{figure}[tb!]
    \centering
    \begin{subfigure}[t]{0.45\linewidth}
        \includegraphics[width=\linewidth]{fig/official_construction_cover.png}
        \subcaption{CARLA Leaderboard V2}
      \end{subfigure}
      \begin{subfigure}[t]{0.45\linewidth}
          \includegraphics[width=\linewidth]{fig/our_constrution_cover.png}\subcaption{Bench2Drive Dataset}
      \end{subfigure}
\caption{\textbf{Distribution of scenario  'ConstructionObstacle' in Town12}\label{fig:construction} We use different colors to represent different routes containing 'ConstructionObstacle'. Bench2Drive has more locations that be able to generate 'ConstructionObstacle'.
}\vspace{-4mm}
\end{figure}



Moreover, to tackle the challenge posed by the long-tail distribution of data from both perception and behavior perspectives, a significant bottleneck in autonomous driving~\cite{jain2021autonomy} (approximately 75\% of the clips in nuScenes only involve the ego vehicle driving straight), we ensure the distribution of weather conditions, landscapes, and behaviors are as uniform as possible. We add more available locations for scenarios compared to the official routes of CARLA Leaderboard V2 as shown in Fig.~\ref{fig:construction}, enhancing the data diversity. Further, we design 5 more  scenarios beyond Leaderboard V2 to enhance behavior diversity as detailed in Appendix~\ref{sec:description-scenario}. We give the
distribution of scenarios and weathers and towns in Appendix~\ref{sec:distribution-town-weather}. As illustrated, Bench2Drive dataset is rich in both perception and behavior diversity.



For data partitioning, we segmented the driving process into short clips, each approximately 150 meters in length and containing a single specific scenario. This segmentation allows for the curriculum learning~\cite{soviany2022curriculum} of individual driving skills. To cater to different computational capabilities, we designed three data subsets: mini (10 clips for debugging and visualization), base (1,000 clips, comparable to nuScenes, suitable for 8xRTX3090 server), and full (10,000 clips for large-scale studies).

\begin{table}[]
\caption{\textbf{Skill Set \& Scenarios}\label{tab:skill}}
\begin{tabular}{l|p{11cm}}
\toprule
\textbf{Skill}  & \textbf{Scenario}                                                                                        \\ \midrule
Merging         & CrossingBicycleFlow, EnterActorFlow, HighwayExit, InterurbanActorFlow, HighwayCutIn, InterurbanAdvancedActorFlow, MergerIntoSlowTrafficV2, MergeIntoSlowTraffic, NonSignalizedJunctionLeftTurn, NonSignalizedJunctionRightTurn, NonSignalizedJunctionLeftTurnEnterFlow, ParkingExit, LaneChange, SignalizedJunctionLeftTurn, SignalizedJunctionRightTurn, SignalizedJunctionLeftTurnEnterFlow                                                                   \\ \midrule
Overtaking      & Accident, AccidentTwoWays, ConstructionObstacle, ConstructionObstacleTwoWays, HazardAtSideLaneTwoWays, HazardAtSideLane, ParkedObstacleTwoWays, ParkedObstacle, VehicleOpenDoorTwoWays                                                                \\ \midrule
Emergency Brake & BlockedIntersection, DynamicObjectCrossing, HardBreakRoute, OppositeVehicleTakingPriority, OppositeVehicleRunningRedLight, ParkingCutIn, PedestrianCrossing, ParkingCrossingPedestrian, StaticCutIn, VehicleTurningRoute, VehicleTurningRoutePedestrian, ControlLoss                                         \\ \midrule
Give Way        & InvadingTurn, YieldToEmergencyVehicle                                                                     \\ \midrule
Traffic Sign    & EnterActorFlow, CrossingBicycleFlow, NonSignalizedJunctionLeftTurn, NonSignalizedJunctionRightTurn, NonSignalizedJunctionLeftTurnEnterFlow, OppositeVehicleTakingPriority, OppositeVehicleRunningRedLight, PedestrianCrossing, SignalizedJunctionLeftTurn, SignalizedJunctionRightTurn, SignalizedJunctionLeftTurnEnterFlow, TJunction, VanillaNonSignalizedTurn, VanillaSignalizedTurnEncounterGreenLight, VanillaSignalizedTurnEncounterRedLight, VanillaNonSignalizedTurnEncounterStopsign, VehicleTurningRoute, VehicleTurningRoutePedestrian \\ \bottomrule
\end{tabular}
\vspace{-3mm}
\end{table}



\subsection{Multi-Ability Evaluation}
\label{sec:ability}
Existing planning benchmarks~\cite{dosovitskiy2017carla,karnchanachari2024towards,gulino2024waymax} assess the performance of AD systems by averaging scores across all provided routes. This approach offers a general overview of driving capabilities but fails to pinpoint specific strengths and weaknesses of different methods. Even worse, existing benchmarks in CARLA like Longest6~\cite{Chitta2023PAMI} and Leaderboard V2~\cite{dosovitskiy2017carla} cover several kilometers, leading to high variance in the driving score metric. This variance arises because the infraction score penalizes errors through cumulative multiplication, which can significantly skew results. For instance, consider three test runs where each achieves 90\% route completion, but the number of red lights run differs: 0, 1, and 2. The corresponding driving scores would be 90, $90*0.7=63$, and $90*0.7*0.7=44.1$, which causes a large standard deviation - 18.9 and thus makes comparison between methods unreliable.

To address these issues, we propose a more granular evaluation framework for all 44 scenarios by designing 5 distinct short routes (around 150 meters in length) per scenario, each featuring different weathers and towns, which result in a total of 220 routes. This approach allows people to assess AD systems' capabilities by isolated skills, leading to a more detailed analysis with reduced variance. Further, we summarize 5 advanced skills for urban driving: Merging, Overtaking, Give Way, Traffic Sign, Emergency Brake as in Table~\ref{tab:skill} and report the score of each skill. The decoupled design provides a clearer insight into which skills are effectively handled by the AD systems and which are not, fostering a more nuanced understanding of system performance.



Formally, the evaluation set consists of 220 routes and each route defines a pair of source location $(x_{\text{src}}, y_{\text{src}})$ and destination location $(x_{\text{dst}}, y_{\text{dst}})$ in one specific town and weather. Given raw sensor inputs (cameras, LiDAR, IMU/GPS, etc) as well as the target waypoints, the ego vehicle should drive from source to the destination location. We design two metrics to evaluate the performance: 
\begin{itemize}[leftmargin=10pt, topsep=0pt, itemsep=1pt, partopsep=1pt, parsep=1pt]
\item \textbf{Success Rate (SR)}: This metric measures the proportion of successfully completed routes within the allotted time and without traffic violations. A route is deemed successful if the ego vehicle reaches its destination without any rule infractions. The success rate is calculated as the ratio of successful routes to the total number of routes, as shown in Equ.~\ref{equ:metric} (left).

\item \textbf{Driving Score (DS)}: This metric follows CARLA~\cite{dosovitskiy2017carla} official metric as reference. It considers both route completion and penalty for infractions. Specifically, it averages the route completion percentages and penalizes infractions based on their severity, as depicted in Equation \ref{equ:metric} (right). The driving score is normalized by the total number of routes from same type or group as well.

\end{itemize}
\begin{equation}
\label{equ:metric}
    \text{Success Rate} = \frac{n_{\text{success}}}{n_{\text{total}}} \quad\quad  \text{Driving Score} = \frac{1}{n_{\text{total}}} \sum\limits_{i=1}^{n_{\text{total}}} \text{Route-Completion}_i * \prod\limits_{j=1}^{n_{i, \text{penalty}}} p_{i, j}
\end{equation} 

where $n_{\text{success}}$ and ${n_{\text{total}}}$ denote the number of successful routes and total samples respectively; $\text{Route-Completion}_i$ representats the percentage of route distance completed for the $i$-th route; $p_{i,j}$ means the $j$-th infraction penalty on the $i$-th route. Please refer to Appendix~\ref{sec:infraction} for details about infraction types and penalties scores. 

Further, beyond the the goal achieving ability of algorithms, we propose the following two metrics to  measure the efficiency and smoothness of driving trajectories:
\begin{itemize}[leftmargin=10pt, topsep=0pt, itemsep=1pt, partopsep=1pt, parsep=1pt]
\item \textbf{Efficiency}:
The CARLA team has implemented a function to check whether the self-driving car's speed is too low. This is determined by comparing the vehicle's speed with nearby vehicle: 
\begin{equation}
    \text{Speed Percentage}=\frac{\text{Ego Vehicle's Speed}}{\text{Average Speed of Nearby Vehicles}}
\end{equation}
This function calculates the speed percentage using the vehicle’s speed and the average speed of nearby vehicles at current frame. CARLA Leaderboard sets four checkpoints per route and checks the ego vehicle's speed when the ego vehicle arrives a checkpoint. Specifically, if the vehicle is faster than nearby vehicles, the driving efficiency would be larger than 100\%. The check results are included as a penalty in the final driving score. However, with only four checkpoints, the vehicle must cover 25\% of the total route distance before reaching the next checkpoint. This leads to a high variance in the penalty values for low speeds, complicating the reflection of driving capabilities in the driving scores. To alleviate this, we increase the number of checkpoints to 20. Speed check is now performed every 5\% of the total route length, and it is excluded from the driving score calculation. The final driving efficiency metric is defined as the average of the speed percentage over all checks. 
\begin{equation}
    \text{Driving Efficiency}=\frac{\sum_i \text{Speed Percentage}\phantom{}_i}{\text{Speed Check Times}}
\end{equation}
If the ego vehicle fails to pass the initial 5\% checkpoint, this route is not included in the final driving efficiency metric calculation. To account for cases where abnormal speed spikes may occur (e.g., when the vehicle falls off the current map layer), speed percentage values exceeding 1000\% are filtered out. 

\textbf{Comfortness}: Comfortness is closely related to human experience and thus requires comparing autonomous driving policy with the behavior of numerous human driving experts to measure it. For this, we follow the popular benchmark nuPlan's~\cite{karnchanachari2024towards} smoothness(also called comfort) protocol, which evaluates ego’s minimum and maximum longitudinal accelerations, the maximum absolute values of lateral acceleration, yaw rate, yaw acceleration, the longitudinal component of jerk, and the maximum magnitude of the jerk vector. These variables are compared to thresholds with default values determined empirically from the examination of nuPlan’s human expert trajectories. Comfortness is measured based on whether these values fall within the upper and lower bounds of the expert values. 
\begin{equation}
\begin{split}
        \text{Frame Variable Smoothness (FVS)} &= \begin{cases} 
\text{True} & \text{if lower bound}\leq p_{\,i} \leq \text{upper bound}, \\
\text{False} & \text{otherwise}
\end{cases} \\
 & p \in \text{smoothness vars}, \, 0\leq i \leq \text{total frames}
\end{split}
\end{equation}
where smoothness variables(vars) include:
longitudinal acceleration -  expert bound: [-4.05, 2.40], maximum absolute lateral acceleration - expert bound: [-4.89, 4.89], yaw rate - expert bound: [-0.95, 0.95], yaw acceleration - expert bound: [-1.93, 1.93], longitudinal component of jerk - expert bound: [-4.13, 4.13], maximum magnitude of jerk vector - expert bound:[-8.37, 8.37].

A trajectory is deemed Smooth only if all smoothness variables meet the smoothness criteria. 
$$\text{Trajectory Smoothness} =\bigwedge_{i=0}^{\text{total frames}}\text{FVS}
$$
In nuPlan, smoothness is determined by frame-by-frame evaluation of these variables over the entire trajectory, which makes it susceptible to local driving behaviors. For example, if a vehicle ahead suddenly brakes, the ego vehicle must also brake abruptly to avoid a collision. Even if the ego’s hard brake behavior is appropriate in this case and its driving is smooth at other times, the entire trajectory could still be judged as unsmooth, leading to unreasonable evaluation results. To mitigate this issue, we segment the entire trajectory at a timestep interval $\small n=20$ for evaluation. 
$$
\text{Segment Smoothness} =\bigwedge_{i=\text{start frame}}^{\text{end frame}}\text{FVS}
$$
The final smoothness metric is defined as the ratio of smooth trajectory segments to the total number of segments. 
$$
\text{Smoothness}=\frac{\text{Number of Smoothness Segments}}{\text{Total Segments}}
$$
Specifically, if the ego vehicle is blocked (speed remains below 0.1 for more than 60 seconds.), resulting in a failure case, this segment is still be considered as smooth because its speed is safe for human. Note that if the total frames of a trajectory are less than 20, the respective route is excluded from the smoothness assessment. 
\end{itemize}



\section{Experiments}


\subsection{Baselines \& Datasets}

To establish a starting point for the community, we have implemented several classic E2E-AD methods in Bench2Drive including:
\begin{itemize}[leftmargin=10pt, topsep=0pt, itemsep=1pt, partopsep=1pt, parsep=1pt]
\item \textbf{UniAD}~\cite{hu2023planning} explicitly conducts perception and prediction and uses Transformer Query to transport information. Together with it, we also implement the commonly used BEVFormer~\cite{li2022bevformer} in Bench2Drive,
\item \textbf{VAD}~\cite{jiang2023vad}  also adopts Transformer Query yet with vectorized scene representation and thus improves efficiency.
\item \textbf{AD-MLP}~\cite{zhai2023ADMLP} simply feeds the ego vehicle's history states into an MLP to predict future trajectories, which is a simple baseline for history state interpolation planner.
\item \textbf{TCP}~\cite{wu2022trajectoryguided} only uses the front cameras and the ego state as inputs to predict both trajectories and control signals. It is a simple yet effective baseline in CARLA v1.
\item \textbf{ThinkTwice}~\cite{jia2023thinktwice} promotes the idea of coarse-to-fine by refining the planning routes in a layer-by-layer manner and distilling the expert features.
\item \textbf{DriveAdapter}~\cite{jia2023driveadapter} proposes a new paradigm to fully unleash the power of expert model by decoupling the learning of perception and planning and connecting the two parts by adapter modules.
\end{itemize}
%We plan to continually integrate cutting-edge methods into this framework with new ones emerging.

Recognizing the varied computational resources available within the community, we have trained these baseline models on the \emph{base} subset (1,000 clips). We use 950 clips for training while leaving 50 clips for open-loop evaluation. We ensure that the validation set contains at least one clip for each of 44 scenarios and the weather distribution is balanced.  AD-MLP and TCP are trained with 1 * A6000 while ThinkTwice, DriveAdapter, UniAD, and VAD are trained with 8 * A100. For the closed-loop evaluation, we run all models in CARLA with the 220 test routes mentioned in Sec.~\ref{sec:ability} and calculate the metric accordingly. Note that some models' might have wrong behaviors in some certain routes (e.g., driving to some buggy location) and cause CARLA to crash without scoring. We treat these routes as 0 score. Please refer to Appendix~\ref{sec:implmentation} for more implementation details. 






\begin{table}[tb!]
\centering
\caption{\textbf{Open-loop and Closed-loop Results of E2E-AD Methods in Bench2Drive} under \textbf{base} training set. Avg. L2 is averaged over the predictions in 2 seconds under 2Hz, similar to UniAD. * denotes expert feature distillation. \label{tab:res}}
\resizebox{1.0\textwidth}{!}{\begin{tabular}{l|c|cccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \textbf{Open-loop Metric} & \multicolumn{4}{c}{\textbf{Closed-loop Metric}}  \\ \cmidrule{2-6} 
                                 &                  Avg. L2 $\downarrow$           & Driving Score $\uparrow$  & Success Rate(\%) $\uparrow$ & Efficiency $\uparrow$ & Comfortness $\uparrow$\\ \midrule
AD-MLP~\cite{zhai2023ADMLP}                           & 3.64              & 18.05     &  0.00  & 48.45 &   22.63  \\ 
UniAD-Tiny~\cite{hu2023planning}                               &  0.80       & 40.73    & 13.18 & 123.92 & 47.04  \\
UniAD-Base~\cite{hu2023planning}                            &  \textbf{ 0.73}          & \textbf{45.81}     & \textbf{16.36} & 129.21 & 43.58   \\

VAD~\cite{jiang2023vad}                              &            0.91                 & 42.35     & 15.00 & \textbf{157.94} & \textbf{46.01}  \\ \midrule
TCP*~\cite{wu2022trajectory}    & 1.70                & 40.70     & 15.00  & 54.26 & 47.80  \\ 
TCP-ctrl*                              & -                 &  30.47    & 7.27  & 55.97 & \textbf{51.51}  \\
TCP-traj*    & 1.70                &  59.90     & 30.00 & 76.54 & 18.08    \\
TCP-traj w/o distillation                              & 1.96                &  49.30     & 20.45  & \textbf{78.78} & 22.96  \\
ThinkTwice*~\cite{jia2023thinktwice}                             & \textbf{0.95}               &  62.44     & 31.23  & 69.33 & 16.22   \\
DriveAdapter*~\cite{jia2023driveadapter}                            & 1.01                &  \textbf{64.22}     & \textbf{33.08} & 70.22 & 16.01   \\ \bottomrule
% TCP-traj (full)                           & Ego State + Front Cameras      &              &      &    \\ \bottomrule
\end{tabular}}
\end{table}


\begin{table}[tb!]
\centering
\caption{\textbf{Multi-Ability Results of E2E-AD Methods} under \textbf{base} training set.  * denotes expert feature distillation.\label{tab:ability}}
\resizebox{1.0\textwidth}{!}{\begin{tabular}{l|ccccc|c}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{5}{c}{\textbf{Ability} (\%) $\uparrow$}                                                                                                                \\ \cmidrule{2-7} 
                                 & \multicolumn{1}{c}{Merging} & \multicolumn{1}{c}{Overtaking} & \multicolumn{1}{c}{Emergency Brake} & \multicolumn{1}{c}{Give Way} & Traffic Sign & \textbf{Mean} \\ \midrule
AD-MLP~\cite{zhai2023ADMLP}        & 0.00        & 0.00           & 0.00        & 0.00         &  4.35    & 0.87         \\
UniAD-Tiny~\cite{hu2023planning}   & 8.89        & 9.33           & 20.00       & 20.00        & 15.43    & 14.73           \\ 
UniAD-Base~\cite{hu2023planning}   & 14.10       & 17.78          & 21.67       &  10.00       & 14.21    & 15.55       \\ 
VAD~\cite{jiang2023vad}            & 8.11       & 24.44         & 18.64       &  20.00       & 19.15    & \textbf{18.07}       \\ \midrule
TCP*~\cite{wu2022trajectory}        & 16.18        & 20.00           & 20.00        &  10.00         & 6.99     & 14.63         \\
TCP-ctrl*                           & 10.29        & 4.44           & 10.00        &  10.00          & 6.45     & 8.23         \\
TCP-traj*                           & 8.89       & 24.29           & \textbf{51.67}       &  40.00        & 46.28    & 34.22        \\ 
TCP-traj w/o distillation                           & 17.14       & 6.67           & 40.00       &  \textbf{50.00}        & 28.72    & 28.51        \\ 
ThinkTwice*~\cite{jia2023thinktwice}                           & 27.38       &  18.42          & 35.82       &  \textbf{50.00}        & 54.23    & 37.17       \\ 
DriveAdapter*~\cite{jia2023driveadapter}                          & \textbf{28.82}       &\textbf{26.38}           & 48.76      &  \textbf{50.00}         & \textbf{56.43}   & \textbf{42.08}        \\ \bottomrule
% \midrule
% TCP-traj (Full)                           \\ 
\end{tabular}}\vspace{-3mm}
\end{table}

\subsection{Results}
In Table~\ref{tab:res} and Table~\ref{tab:ability}, we compare baselines E2E-AD methods with both open-loop and closed-loop evaluation, which lead to the following findings:

\noindent\textbf{Open-loop metric could indicate model convergence  but it fails for advanced comparison.}.  AD-MLP has a high L2 error and performs extremely bad in closed-loop evaluation while VAD has a low L2 error and a decent closed-loop performance. \emph{It shows that we could use L2 error to verify the convergence and fitting status of neural networks}, i.e., when the L2 error is very high, there should be something wrong within the system. In this case, AD-MLP does not use raw sensors, which is similar to drive blindly and thus  infeasible to fit the dataset. Notably, different from findings in nuScenes~\cite{nuscenes}, AD-MLP fails to achieve decent L2 error in Bench2Drive, due to the better behavior diversity as shown in Fig.~\ref{fig:future-location}.
On the other hand, UniAD-base has a lower L2 error compared to VAD yet with worse closed-loop performance, aligning with findings in~\cite{Dauner2023CORL,Li2023IsES}. Open-loop evaluation ignore the issues including distribution shift~\cite{ross2011reduction} and causal confusion~\cite{de2019causal,jia2023driveadapter} and thus fails to give meaningful comparsion for models with good fitting of dataset, demonstrating the importance of closed-loop evaluation. For efficiency and smoothness, we could observe that AD-MLP has the lowest efficiency due to its quick failure and stuck. UniAD has higher efficiency and smoother trajectories compared to TCP-traj, demonstrating the effectivenes of the UniAD's post optimization for the planning head.

\noindent\textbf{Expert feature distillation offers important guidance.}  As pointed out in~\cite{chen2020learning,zhang2021roach}, due to the high-dimensional input space of AD, i.e., multiple images and point clouds, E2E-AD methods tend to overfit. The features from expert, which already possesses strong driving knowledge, could be helpful to mitigate the issue by distillation. As a result, methods (TCP/ThinkTwice/DriveAdapter) with expert feature distillation outperforms those without (VAD/UniAD) by a large margin. From the comparison between TCP-traj with and without distillation, we could observe similar trend. However, in the real world setting, it could be difficult to obtain expert features, which worths further study. 

\noindent\textbf{Interactive behaviors are difficult to learn.} All models' scores of skills regarding strong interaction (Merging, Overtaking, and Emergency Brake) are unsatisfying. It might come from two perspectives: (I) Long-tail issue. Even though we ensure that the number of clips for different scenarios are similar, there are only a few frames within one clip are about interactive behaviors. As a result, it might be challenging for the learning. (II) Imitation learning paradigm.  Direct supervised training of control signals or trajectories might fail to give guidance regarding the gaming, thinking, and reasoning process of interaction. More advanced training paradigms could be a promising direction.

\subsection{Case Analysis} We conduct visualizations and upload the results to \url{https://github.com/Thinklab-SJTU/Bench2DriveZoo/blob/uniad/vad/analysis/analysis.md}. For all five abilities, we choose some representative scenarios to visualize, where some baselines success and some baselines fail for the ease of comparison and analysis. We give the corresponding failure analysis so that the users and practioners could have a sense about the pros, cons, and future works of existing E2E-AD methods. 

\begin{figure}[tb!]
    \centering
    \begin{subfigure}{0.45\linewidth}
        \includegraphics[width=\linewidth]{fig/nus_traj_distribution.PNG}
        \caption{\textbf{nuScenes}~\cite{nuscenes} dataset drawn by~\cite{Li2023IsES}.}
      \end{subfigure}
      \begin{subfigure}{0.4\linewidth}
          \includegraphics[width=\linewidth]{fig/b2d_traj_distribution.pdf}
          \caption{Bench2Drive Dataset}
      \end{subfigure}
    \caption{\textbf{Distribution of ego vehicle's future location.} Bench2Drive  possesses more turning trajectories, indicating better action diversity and thus providing better training data and having less gap between open-loop and closed-loop evaluation.\label{fig:future-location}}\vspace{-6mm}
\end{figure}





\section{Conclusion}
In this work, we present Bench2Drive, a new benchmark tailed for closed-loop evaluation of end-to-end autonomous driving methods. We open source a fully-annotated large-scale dataset as the official training set and a multi-ability evaluation toolkit for the granular driving skill assessment. State-of-the-art E2E-AD methods are tested in Bench2Drive with their pros and cons evaluated, which provides insights for the future direction.

\noindent\textbf{Limitations}:  Since the rendering of simulation in CARLA has gaps compared to real world, utilizing real world datasets could be complementary as done in the concurrent work - NAVSIM~\cite{Dauner2024ARXIV}. Actually, there is a dilemma in the field for the evaluation of end-to-end autonomous driving algorithms:

\begin{table}[!h]
\centering
\begin{tabular}{lcc}
\toprule
Source of Images     & Pros      & Cons          \\ \midrule
Real World Datasets  & Realistic & Non-Reactive  \\ 
Simulation Rendering & Reactive  & Cartoon Style \\ \bottomrule
\end{tabular}
\end{table}

Generative models like diffusion models~\cite{ho2020denoising} might have the potential to provide realistic and reactive rendering, with some pioneering works in the field~\cite{yang2024genad,gao2024vista,zhou2024simgen}. However, the illusion and artifact issue of diffusion requires further exploration.


\noindent\textbf{Social Impact}: The deployment of AD systems holds immense potential to revolutionize transportation, but it also brings significant ethical and safety concerns. Bench2Drive could serve as a platform for rigorously validating the capabilities of AD systems in a controlled and simulated environment, helping to identify potential flaws before real-world deployment. One of the primary risks is the simulation-reality gap—the difference between how an AD system performs in simulation versus in the real world. Simulations have the difficulties to fully replicate the complexities and unpredictability of real-world driving conditions. There is a risk that an AD system might perform well in simulation but fail in real-world scenarios due to unmodeled factors like rare edge cases, unexpected human behaviors, or varying environmental conditions. Bench2Drive is intended to complement, not replace, real-world testing, and it is crucial to emphasize that simulation is one part of a broader validation process that must include extensive on-road testing.

\newpage


% \begin{ack}
% \jxs{TODO}
% Use unnumbered first level headings for the acknowledgments. All acknowledgments
% go at the end of the paper before the list of references. Moreover, you are required to declare
% funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
% More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2024/PaperInformation/FundingDisclosure}.
% You can use the \texttt{ack} environment provided in the style file. As opposed to the main NeurIPS track, acknowledgements do not need to be hidden.
% \end{ack}

{
\small

% \bibliographystyle{ieeenat_fullname}
\bibliographystyle{unsrt}
\bibliography{main}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Checklist}

%%% BEGIN INSTRUCTIONS %%%
The checklist follows the references.  Please
read the checklist guidelines carefully for information on how to answer these
questions.  For each question, change the default \answerTODO{} to \answerYes{},
\answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
justification to your answer}, either by referencing the appropriate section of
your paper or providing a brief inline description.  For example:
\begin{itemize}
  \item Did you include the license to the code and datasets? \answerYes{See Section~\ref{sec:method}.}
  % \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
  % \item Did you include the license to the code and datasets? \answerNA{}
\end{itemize}
Please do not modify the questions and only use the provided macros for your
answers.  Note that the Checklist section does not count towards the page
limit.  In your paper, please delete this instructions block and only keep the
Checklist section heading above along with the questions/answers below.
%%% END INSTRUCTIONS %%%

\begin{enumerate}

\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{}
  \item Did you describe the limitations of your work?
    \answerYes{}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerYes{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}

\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerNA{}
	\item Did you include complete proofs of all theoretical results?
    \answerNA{}
\end{enumerate}

\item If you ran experiments (e.g. for benchmarks)...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerYes{}
	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerNo{}
	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerYes{}
\end{enumerate}

\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{}
  \item Did you mention the license of the assets?
   \answerYes{}
  \item Did you include any new assets either in the supplemental material or as a URL?
   \answerYes{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
     \answerNA{}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
     \answerNA{}
\end{enumerate}

\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerNA{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerNA{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerNA{}
\end{enumerate}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix



\section{Details of Data Collecting}
\label{sec:data}
The collection of data is a mix of automatic pipelines and manual checking. We give details below:


\noindent\textbf{Route}: We use the expert model Think2Drive to run on the predefined route files and only keep those without infractions. We design a traversal algorithm over all maps to determine whether a scenario could be triggered, aiming to cover towns as much as possible. The balance of weather, towns, and scenarios are ensured by manually checking. The behaviors and rendering of CARLA sometimes could be buggy as shown in Fig.~\ref{fig:render-bug} and we manually filter those bad clips.

\noindent\textbf{Annotations}: We utilize CARLA's official APIs to collect annotations. Notably, there are several bugs within the APIs: (I) All pedestrians' speed value are 0 from the API. We manually calculate their speed by differentiating during training of baseline methods. (II) The returned value of Speedometer and IMU could be None. We pad these values with 0 during training. (III) Some stop signs in CARLA are on the ground and thus there is no bounding box. To compensate this, We record all stop signs with rectangles to denote their trigger volume.  (IV) Some static vehicles' rotation and location are wrong by API. Thus, we use the correct center and extent to obtain their 3D bounding boxes.


\noindent\textbf{Object Class}: Considering different attributes of different objects, we categorize all objects into four main types and store them group by group: Vehicle, Traffic Sign, Traffic Light, and Pedestrian. Vehicles are further subdivided into static and dynamic vehicles. Static vehicles remain stationary throughout the entire scenario and are distinguished by a unique actor identifier obtained from "static.prop.mesh".  For Traffic Signs, they consist of speed\_limit\_sign, stop\_sign, yield\_sign, warning\_sign (including warning construction, traffic warning, and warning accident), dirt\_debris, and cone. Notably, signs involving trigger\_volume, such as speed\_limit\_sign, stop\_sign, and yield\_sign, has trigger volume where we store the rectangle as well. The coordinates for warning signs and cones are obtained from the center and extent of their actor class, while dirt\_debris requires additional conversion due to inaccurate coordinates. For Traffic Lights, the trigger volume coordinates for traffic signs and lights are relative to the actor, which requires extra transformation. Each traffic sign/light consists of two parts: the pole and the light/sign itself while the bounding boxes from API is only the lights/signs.


\noindent\textbf{Coordinate System}: Unlike the Y-down right-hand system used by Nuscenes, CARLA employs the Unreal Engine coordinate system, which is a Z-up left-hand coordinate system. In Compass, orientation with regard to the North ([0.0, -1.0, 0.0] in Unreal Engine) means the standard yaw angle in the left-hand system is theta in the compass minus 1/2 pi. In rare cases, this may result in NaN values, which need to be manually filtered.

\noindent\textbf{Map Information}: The HD-Map is organized into road\_ids with lane\_index. Each lane includes the world coordinates and orientations of points, lane type, color identifiers and adjacent road\_ids-lane\_ids (left, right and connected road ids and lane ids), as well as topological structures (e.g., 'Junction', 'Normal', 'EnterNormal', 'EnterJunction', 'PassNormal', 'PassJunction', 'StartJunctionMultiChange', or 'StartNormalMultiChange'). The Trigger\_Volumes represent the trigger areas for signs, where 'Points' specify the vertices' locations of the trigger volume, 'Type' can be 'StopSign' or 'TrafficLight', and 'ParentActor\_Location' provides details on the location of the parent actor associated with the trigger volume.

\noindent\textbf{Data Compression}
To reduce file size, following~\cite{Jaeger2023ICCV}, we adopted compressed data format. Images are compressed using JPG with the quality$=20$. To avoid train-val gap during closed-loop evaluation, we also use in-memory JPG compression and decompression during inference. Semantic segmentation and depth data are stored as PNG files. We use a specialized algorithm called laszip to compress our LiDAR point clouds. JSON files are compressed using GZIP.

\section{Distribution of Scenarios, Towns and Weathers}
\label{sec:distribution-town-weather}
As shown in  Fig.~\ref{fig:town-weather-distribution}, the distribution of weathers are nearly uniform while the distribution of town is dominated by Town12 and Town13. The imbalance comes from two perspectives: (I) New towns, like Town12 and Town13, are designed on purpose by CARLA team to be much larger than old towns so that the community could explore applications in city-level scene. Thus, we collect more data in larger towns for more diverse landscapes. (II) Lots of new scenarios in Leaderboard v2 are designed recently and thus old towns do not support the layouts required by new scenarios. For example, parting exit requires the existence of other parked vehicles.  As a result, we have to collect more data in new towns to ensure the balance of  scenario types. 


\begin{figure}[tb!]
    \centering
    \begin{subfigure}{1.0\linewidth}
        \centering
    \includegraphics[width=\linewidth]{fig/full-scenario.png}
    \end{subfigure}
    \caption{\textbf{Scenario Distribution of Bench2Drive Dataset}    \label{fig:scenario-distribution}}
\end{figure}

\begin{figure}[!tb]
    \centering
    \includegraphics[width=0.7\linewidth]{fig/buggy_rendering.png}
    \caption{\textbf{Bugged Rendering of CARLA}.}
    \label{fig:render-bug}
\end{figure}

\begin{figure}[tb!]
    \centering
    \begin{subfigure}{\linewidth}
      % \label{subfig:worldmodel learning}
        \includegraphics[width=\linewidth]{fig/full-wt.pdf}
      \end{subfigure}
    \caption{\textbf{Town and Weather Distribution of Bench2Drive Dataset}}
    \label{fig:town-weather-distribution}
\end{figure}



\section{Implementation Details of Baselines}
\label{sec:implmentation}
For all baseline E2E-AD methods, we strictly follow their official open-sourced code, environments, and configs.  There are a few modifications: (I) For methods with object detection module, we change the detection classes according to CARLA\footnote{\url{https://carla.readthedocs.io/en/latest/catalogue_vehicles/}}. (II) Data collection of Bench2Drive is in 10Hz while nuScenes with bounding boxes is in 2Hz. Due to Bench2Drive's longer clips, Bench2Drive-base has approximately 10x frames compared to nuScenes yet with higher redundancy. For computationally demanding methods like UniAD/VAD/ThinkTwice/DriveAdapter, we train $\frac{1}{10}$ epochs compared to the original version. We observe similar level of loss due to similar number of training steps.  (III) Since ThinkTwice and DriveAdapter require expert's BEV features, we use Think2Drive expert to regenerate those expert feature. Additionally, for fair comparison with other methods, we modify both of them into 6 cameras without LiDAR.

\section{Training and Evaluation Resource Requirements}
We report resource requirements of training and evaluating baselines. Note that the evaluation time could be linearly speed up with more GPUs to parallely evaluate on more routes.

\begin{table}[]
\caption{\textbf{Resource requirements for training on base set and evaluation on 220 routes.} The training of UniAD consists of BEVFormer, stage1, and stage2.\label{tab:resource}}
\centering
\begin{tabular}{lll}
\toprule
           & Training             & Evaluation        \\ \midrule
AD-MLP     & 1 A600 * 1 day       & 4 A6000 * 6 hours \\ 
TCP        & 1 A600 * 1 day       & 4 A6000 * 6 hours \\ 
VAD        & 8 H800 * 5 days      & 8 H800 * 2 days   \\ 
UniAD-base & 8 H800 * (3+3+3)=9 days & 8 H800 * 2 days   \\ \bottomrule
\end{tabular}
\end{table}

\section{Behavior Model of NPC Agents}


In CARLA, three behavior types are preset in   \href{https://carla.readthedocs.io/en/latest/adv_agents/#behavior-types}{CARLA Agent}: \textbf{cautious}, \textbf{normal} and \textbf{aggressive}. These behavior types govern the driving actions of NPCs, influencing factors such as speed, responses to other vehicles, and safety protocols. The key parameters for each behavior mode include:

\begin{itemize}
    \item \textbf{max\_speed}: Sets the maximum speed(km/h) that an NPC vehicle can reach.
    \item \textbf{speed\_lim\_dist}: Value in km/h that defines how far your vehicle's target speed will be from the current speed limit
    \item \textbf{speed\_decrease}: Controls the deceleration of the NPC when approaching a slower vehicle ahead.
    \item \textbf{safety\_time}: Estimates the time to a collision if the vehicle in front suddenly brakes.
    \item \textbf{min\_proximity\_threshold}: Defines the minimum distance before the NPC takes actions such as evasive maneuvers or tailgating.
    \item \textbf{braking\_distance}: The distance at which the NPC performs an emergency stop to avoid a collision.
    \item \textbf{tailgate\_counter}: A counter that prevents the NPC from initiating a new tailgating action too soon after the last one.
\end{itemize}

These behavior designs interact with the ego (self-driving) vehicle, ensuring that NPCs respond to the presence and actions of the ego vehicle in the simulation. The parameters for different behavior styles are as follows,

\begin{table}[h!]
\centering
\caption{Comparison of Cautious, Normal, and Aggressive behavior parameters.}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Parameter}               & \textbf{Cautious} & \textbf{Normal} & \textbf{Aggressive} \\ \hline
\textbf{max\_speed (km/h)}              & 40                & 50              & 70                  \\ \hline
\textbf{speed\_lim\_dist (km/h)}        & 6                 & 3               & 1                   \\ \hline
\textbf{speed\_decrease (km/h)}         & 12                & 10              & 8                   \\ \hline
\textbf{safety\_time (seconds)}            & 3                 & 3               & 3                   \\ \hline
\textbf{min\_proximity\_threshold (meters)} & 12                & 10              & 8                   \\ \hline
\textbf{braking\_distance (meters)}       & 6                 & 5               & 4                   \\ \hline
\textbf{tailgate\_counter (times)}       & 0                 & 0               & -1                  \\ \hline
\end{tabular}

\end{table}


The rule-based behavior decision algorithm in \href{https://github.com/carla-simulator/carla/blob/dev/PythonAPI/carla/agents/navigation/behavior_agent.py}{behavior\_agent.py} for non-player character (NPC) vehicles is shown below:
\begin{algorithm}
\caption{Non-Player Character (NPC) Vehicles Behavior Decision}
\begin{algorithmic}[1]
\STATE \textbf{Update vehicle's surrounding information}
\IF{red light \textbf{or} stop sign detected}
    \RETURN \textbf{\texttt{emergency\_stop()}}
\ENDIF

\IF{pedestrian detected \textbf{and} within braking distance}
    \RETURN \textbf{\texttt{emergency\_stop()}}
\ENDIF

\IF{other vehicle nearby}
    \IF{within braking distance}
        \RETURN \textbf{\texttt{emergency\_stop()}}
    \ELSE
        \RETURN \textbf{\texttt{car\_following()}}
    \ENDIF

\ELSIF{ego vehicle at intersection \textbf{and} (turn left or turn right)}
    \STATE \textbf{\texttt{adjust\_speed\_limit()}}
    \RETURN PID\_Control

\ELSIF{in \textbf{normal driving conditions}}
    \STATE \textbf{\texttt{maintain\_speed\_limit()}}
    \RETURN PID\_Control
\ENDIF
\end{algorithmic}
\end{algorithm}

The behavior of walkers/pedestrians is characterized by their consistent adherence to the route line at a constant speed, with the inability to walk backward. In autonomous driving scenarios, pedestrian safety is of utmost importance. Pedestrians have the highest priority when autonomous vehicles interact with humans, and autonomous vehicles should learn to give way to pedestrians regardless of traffic conditions.


\section{Details about Infraction Score}
\label{sec:infraction}
In Table~\ref{tab:infraction}, we gives the penalty score of each infraction designed by Leaderboard v2.

\begin{table}[]
\centering
%\small
\caption{\textbf{Infraction Types \& Penalty}. Following \url{https://leaderboard.carla.org/}.\label{tab:infraction}}
\begin{tabular}{lll}
\toprule
\textbf{Infraction}  & \textbf{Penalty} & \textbf{Note}                                                               \\ \midrule
Pedestrian Collision & 0.50             & Punished every infraction.                                                   \\
Vehicles Collision   & 0.60             & Punished every infraction.                                                   \\
Other Collision      & 0.65             & Punished every infraction.                                                   \\
Running Red Light    & 0.70             & Punished every infraction.                                 \\
Scenario Timeout     & 0.70             & Fail to pass certain scenarios in 4 minutes.                                 \\ 
Too Slow             & 0.70             & Fail to maintain a suitable speed with surrounding vehicle.                 \\ 
No Give Way          & 0.70             & Failure to yield to emergency vehicle.                                       \\ 
Off-road             & -                & Not considered in route completion.                                          \\ 
Route Deviation      & -                & Deviates more than 30 meters. Shutdown immediately. \\
Agent Blocked        & -                & No action for 180 seconds. Shutdown immediately.                            \\
Route Timeout        & -                & Exceed the maximum time limit. Shutdown immediately.           \\ \bottomrule
\end{tabular}
\end{table}





\section{Description of Scenarios}
\label{sec:description-scenario}
Bench2Drive provides 44 corner scenarios, extended from CARLA Leaderboard V2. We give details about them below:



\textbf{\textit{1. ControlLoss}} 

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/ControlLoss.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle loses control due to bad conditions on the road and it must recover, coming back to its original lane.
\end{minipage}

\textbf{\textit{2. ParkingExit}}

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/ParkingExit.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle must exit a parallel parking bay into a flow of traffic.
\end{minipage}

\textbf{\textit{3. ParkingCutIn}}

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/ParkingCutIn.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle must slow down or brake to allow a parked vehicle exiting a parallel parking bay to cut in front. 
\end{minipage}

\textbf{\textit{4. StaticCutIn}}

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/StaticCutIn.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle must slow down or brake to allow a vehicle of the slow traffic flow in the adjacent lane to cut in front. 
Compared to \textit{ParkingCutIn}, there are more cars in the adjacent lane and any one of them may cut in. 
\end{minipage}

\textbf{\textit{5. ParkedObstacle}}

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/ParkedObstacle.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle encounters a parked vehicle blocking part of the lane and must perform a lane change into traffic moving in the same direction to avoid it. 
\end{minipage}

\textbf{\textit{6. ParkedObstacleTwoWays}}

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/ParkedObstacleTwoWays.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The '\textit{TwoWays}' version of \textit{ParkedObstacle}. 
The ego vehicle encounters a parked vehicle blocking the lane and must perform a lane change into traffic moving in the opposite direction to avoid it. 
\end{minipage}

\textbf{\textit{7. Construction}}

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/Construction.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle encounters a construction site blocking and must perform a lane change into traffic moving in the same direction to avoid it. 
Compared to \textit{ParkedObstacle}, the construction occupies more width of the lane. 
The ego vehicle has to completely deviate from its task route temporarily to bypass the construction zone. 
\end{minipage}

\textbf{\textit{8. ConstructionTwoWays}}

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/ConstructionTwoWays.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The '\textit{TwoWays}' version of \textit{Construction}. 
\end{minipage}

\textbf{\textit{9. Accident}} 

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/Accident.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle encounters multiple accident cars blocking part of the lane and must perform a lane change into traffic moving in the same direction to avoid it. 
Compared to \textit{ParkedObstacle} and \textit{Construction}, these accident cars occupy more length along the lane. 
The ego vehicle has to completely deviate from its task route for a longer time to bypass the accident zone. 
\end{minipage}

\textbf{\textit{10. AccidentTwoWays}}

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/AccidentTwoWays.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The '\textit{TwoWays}' version of \textit{Accident}. 
Compared to \textit{ParkedObstacleTwoWays} and \textit{ConstructionTwoWays}, there is a much shorter time window for the ego vehicle to bypass the route obstacles (i.g. accident cars). 
\end{minipage}

\textbf{\textit{11. HazardAtSideLane}}

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/HighzardAtSideLane.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle encounters a slow-moving hazard blocking part of the lane. 
The ego vehicle must brake or maneuver next to a lane of traffic moving in the same direction to avoid it. 
\end{minipage}

\textbf{\textit{12. HazardAtSideLaneTwoWays}}

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/HizardAtSideLaneTwoways.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle encounters a slow-moving hazard blocking part of the lane. The ego vehicle must brake or maneuver to avoid it next to a lane of traffic moving in the opposite direction. 
\end{minipage}

\textbf{\textit{13. VehiclesDooropenTwoWays}}

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/VihicleDoorOpen.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle encounters a parked vehicle opening a door into its lane and must maneuver to avoid it. 
\end{minipage}

\textbf{\textit{14. DynamicObjectCrossing}}

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/DynamicCrossing.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
A walker or bicycle behind a static prop crosses the road suddenly when the ego vehicle is close to the prop. 
The ego vehicle must make a hard brake promptly. 
\end{minipage}

\textbf{\textit{15. ParkingCrossingPedestrian}}

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/ParkingObstaclePedestrainCrossing.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle encounters a pedestrian emerging from behind a parked vehicle and advancing into the lane. 
The ego vehicle must brake or maneuver to avoid it. 
Compared to \textit{DynamicObjectCrossing}, the pedestrian is closer to the road and the ego vehicle has to act more timely. 
\end{minipage}

\textbf{\textit{16. HardBrake}}

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/HardBrake.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The leading vehicle decelerates suddenly and the ego vehicle must perform an emergency brake or an avoidance maneuver. 
\end{minipage}

\textbf{\textit{17. YieldToEmergencyVehicle}}

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/YieldToEmergency.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle is approached by an emergency vehicle coming from behind. 
The ego vehicle must maneuver to allow the emergency vehicle to pass.
\end{minipage}

\textbf{\textit{18. InvadingTurn}}

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/InvadingTurn.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
When the ego vehicle is about to turn right, a vehicle coming from the opposite lane invades the ego's lane, forcing the ego to move right to avoid a possible collision. 
\end{minipage}

\textbf{\textit{19. PedestrainCrossing}}

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/PedestrainCrossing.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
While the ego vehicle is entering a junction, a group of natural pedestrians suddenly cross the road and ignore the traffic light. 
The ego vehicle must stop and wait for all pedestrians to pass even though there is a green traffic light or a clear junction. 
\end{minipage}

\textbf{\textit{20. VehicleTurningRoutePedestrian}}

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/vihicleTurnRoute.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
While performing a maneuver, the ego vehicle encounters a pedestrian crossing the road and must perform an emergency brake or an avoidance maneuver. 
\end{minipage}

\textbf{\textit{21. VehicleTurningRoute}}
While performing a maneuver, the ego vehicle encounters a bicycle crossing the road and must perform an emergency brake or an avoidance maneuver. 
Compared to \textit{VehicleTurningRoutePedestrian}, the bicycle moves faster and the ego has to brake earlier. 

\textbf{\textit{22. BlockedIntersection}} 

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/BlockInstersection.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
While performing a maneuver, the ego vehicle encounters a stopped vehicle on the road and must perform an emergency brake or an avoidance maneuver. 
\end{minipage}

\textbf{\textit{23. SignalizedJunctionLeftTurn}} 

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/SignalizedLeftTurn.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle is performing an unprotected left turn at an intersection, yielding to oncoming traffic. 
\end{minipage}

\textbf{\textit{24. SignalizedJunctionLeftTurnEnterFlow}} 

The ego vehicle is performing an unprotected left turn at an intersection, merging into opposite traffic. 

\textbf{\textit{25. NonSignalizedJunctionLeftTurn}} 

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/NonSignalizedJunctionLeftTurn.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
Non-signalized version of \textit{SignalizedJunctionLeftTurn}. 
The ego has to negotiate with the opposite vehicles without traffic lights. 
\end{minipage}

\textbf{\textit{26. NonSignalizedJunctionLeftTurnEnterFlow}} 

Non-signalized version of \textit{SignalizedJunctionLeftTurnEnterFlow}. 

\textbf{\textit{27. SignalizedJunctionRightTurn}} 

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/SignalizedRightTurn.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle is turning right at an intersection and has to safely merge into the traffic flow coming from its left. 
\end{minipage}
% \textbf{\textit{10. NonSignalizedJunctionRightTurn}} 
% Non-signalized version of \textit{SignalizedJunctionRightTurn}. 
% The ego has to negotiate with the traffic flow without traffic lights. 

\textbf{\textit{28. NonSignalizedJunctionRightTurn}} 

Non-signalized version of \textit{SignalizedJunctionRightTurn}. 
The ego has to negotiate with the traffic flow without traffic lights. 


\textbf{\textit{29. EnterActorFlows}} 

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/RunRedLight.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
A flow of cars runs a red light in front of the ego when it enters the junction, forcing it to react (interrupting the flow or merging into the flow). 
These vehicles are 'special' ones such as police cars, ambulances, or firetrucks. 
\end{minipage}

\textbf{\textit{30. HighwayExit}} 

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/HighwayExit.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle must cross a lane of moving traffic to exit the highway at an off-ramp. 
\end{minipage}

\textbf{\textit{31. MergerIntoSlowTraffic}} 

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/MergeIntoSlowTrafficFlow.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle must merge into a slow traffic flow on the off-ramp when exiting the highway. 
\end{minipage}

\textbf{\textit{32. MergerIntoSlowTrafficV2}} 

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/MergeIntoSlowTrafficFlowV2.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle must merge into a slow traffic flow coming from the on-ramp when driving on highway roads. 
\end{minipage}

\textbf{\textit{33. InterurbanActorFlow}} 

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/InteruptActorFlow.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle leaves the interurban road by turning left, crossing a fast traffic flow. 
\end{minipage}

\textbf{\textit{34. InterurbanAdvancedActorFlow}} 

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/InteruptAdvancedFlow.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle incorporates into the interurban road by turning left, first crossing a fast traffic flow, and then merging into another one. 
\end{minipage}

\textbf{\textit{35. HighwayCutIn}} 

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/HighWayCutIn.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle encounters a vehicle merging into its lane from a highway on-ramp. 
The ego vehicle must decelerate, brake, or change lanes to avoid a collision. 
\end{minipage}

\textbf{\textit{36. CrossingBicycleFlow}} 

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/CrossBicycleFlow.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle needs to perform a turn at an intersection yielding to bicycles crossing from either the left. 
\end{minipage}

\textbf{\textit{37. OppositeVehicleRunningRedLight}} 

\begin{minipage}{0.3\textwidth}
\includegraphics[width=\textwidth]{fig_scenario/RunRedLight.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
The ego vehicle is going straight at an intersection but a crossing vehicle runs a red light, forcing the ego vehicle to avoid the collision.
\end{minipage}

\textbf{\textit{38. OppositeVehicleTakingPriority}} 

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/OppositeTakePriority.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
Non-signalized version of \textit{OppositeVehicleTakingPriority}. 
\end{minipage}

% \textbf{\textit{39. VinillaTurn}}

% \begin{minipage}{0.3\textwidth}
% \hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/VinillaTurn.png}
% \end{minipage}
% \begin{minipage}{0.67\textwidth}
% A basic scenario for the ego vehicle to learn the basic traffic rules, e.g. stop signs and traffic lights. 
% \end{minipage}
\textbf{\textit{39. VinillaNonSignalizedTurn}}

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/VinillaNonSignalizedTurn.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
A basic scenario for the ego vehicle to learn to pass through a non-signalized junction (without traffic signs and traffic lights). 
\end{minipage}

\textbf{\textit{40. VinillaNonSignalizedTurnEncounterStopsign}}

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/VinillaNonSignalizedTurnEncounterStopsign.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
A basic scenario for the ego vehicle to learn to stop and start at stop signs. 
\end{minipage}

\textbf{\textit{41. VinillaSignalizedTurnEncounterGreenLight}}

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/VinillaSignalizedTurnEncounterGreenLight.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
A basic scenario for the ego vehicle to learn to pass through the signalized junction. 
\end{minipage}

\textbf{\textit{42. VinillaSignalizedTurnEncounterRedLight}}

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/VinillaSignalizedTurnEncounterRedLight.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
A basic scenario for the ego vehicle to learn to pass through the signalized junction when the traffic light changes from red to green.  
\end{minipage}

\textbf{\textit{43. LaneChange}}

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/LaneChange.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
A basic scenario for the ego vehicle to learn to change lanes and avoid collision. 
\end{minipage}

\textbf{\textit{44. TJunction}}

\begin{minipage}{0.3\textwidth}
\hspace{0.8cm}\includegraphics[width=0.5\textwidth]{fig_scenario/TJunction.png}
\end{minipage}
\begin{minipage}{0.67\textwidth}
A basic scenario for the ego vehicle to learn to pass through a T-junction.  
\end{minipage}



\section{Author Statement}
We bear all responsibility in case of violation of rights, etc., and confirm the license of data, codes, and checkpoints as in Sec.~\ref{sec:method}.

\section{License}
 All data, codes, and checkpoints are in GitHub and Huggingface under Apache License 2.0.


\section{Datasheet}
\subsection{Motivation}
\noindent\textbf{For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.} We build the benchmark to fulfill the need of comprehensive and realistic testing environments for Full Self-Driving (FSD). The primary task is end-to-end autonomous driving. Existing benchmarks failed to provide a closed-loop granular assessment of driving skills for E2E-AD methods. 
\vspace{4mm}

\noindent\textbf{Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g.,
company, institution, organization)?} This dataset is curated by Xiaosong Jia, Zhenjie Yang, Qifeng Li, Zhiyuan Zhang, Junchi Yan from ReThinkLab with School of AI and Department of CSE, Shanghai Jiao Tong University.
\vspace{4mm}

% \noindent\textbf{Who funded the creation of the dataset?}
% \jxs{TODO}
% \vspace{4mm}

\subsection{Distribution}

\noindent\textbf{Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?}
Yes.
\vspace{4mm}

\noindent\textbf{How will the dataset be distributed (e.g., tarball on website, API, GitHub)? }
 All data, codes, and checkpoints are in GitHub (\url{https://github.com/Thinklab-SJTU/Bench2Drive}) and Huggingface (\url{https://huggingface.co/datasets/rethinlab/Bench2Drive}).
\vspace{4mm}

\subsection{Maintenance}

\noindent\textbf{Who will be supporting/hosting/maintaining the dataset?}
All authors and potentially new members of ReThinLab in Shanghai Jiao Tong University led by Prof. Junchi Yan.
\vspace{4mm}

\noindent\textbf{How can the owner/curator/manager of the dataset be contacted (e.g., email address)?}
Please contact Xiasong Jia (jiaxiaosong@sjtu.edu.cn) and Junchi Yan (yanjunchi@sjtu.edu.cn).
\vspace{4mm}

\noindent\textbf{Is there an erratum? }
No erratum as of submission.
\vspace{4mm}

\noindent\textbf{Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?}
Yes, we will maintain the dataset.
\vspace{4mm}

\noindent\textbf{Will older versions of the dataset continue to be supported/hosted/maintained?}
Yes.
\vspace{4mm}

\noindent\textbf{If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?}
Yes, they could follow the guide in \url{https://github.com/Thinklab-SJTU/Bench2Drive}.
\vspace{4mm}

\subsection{Composition}

\noindent\textbf{What do the instances that comprise the dataset represent?}
One basic instance is one clip. Each clip contains hundreds or thousands of frames in 10 Hz with raw sensor information and annotations..
\vspace{4mm}

\noindent\textbf{How many instances are there in total (of each type, if appropriate)?}
There are 13638 clips in Bench2Drive.
\vspace{4mm}

\noindent\textbf{Are relationships between individual instances made explicit? }
Yes. Each clip is annotated with scenario types and locations. Clips could have the same scenario type or nearby location.
\vspace{4mm}

\noindent\textbf{Are there recommended data splits (e.g., training, development/validation, testing)?}
Yes. In the github repo.
\vspace{4mm}

\noindent\textbf{Is the dataset self-contained, or does it link to or otherwise rely on external resources?}
Self-contained.
\vspace{4mm}

\subsection{Collection Process}
\noindent\textbf{Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?}
All data is collected in CARLA automatically The collection code is written by authors. 
\vspace{4mm}

\subsection{Use}
\noindent\textbf{What (other) tasks could the dataset be used for?} With existing annotations, the dataset could also be used to conduct 3D object detection, semantic segmentation, instance segmentation, point cloud segmentation, depth estimation, tracking, motion prediction.
\vspace{4mm}

\end{document}
